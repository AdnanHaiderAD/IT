---Character Statistics---
1.
H(Xn) = sum [- P(xn)log(Pxn)  ] here sum sum stands for summation
P(xn) = cn/N where cn is the number of time xn occurs in the file and N is the total number of alphabets in the file

Code written :
#! /usr/bin/python
import re,math
def unigrammodel(file):
	f = open(file,'r')
	try: 
		content= f.read()
		letters =map(chr,range(97,123))
		unigramcounts =[]
		sizeOFcontent=len(content)
		
		for l in letters:
			unigramcounts.append(len(re.findall(l,content)))#counts the number of times each letter in the alphabet occurs in the file
		unigramcounts.append(sizeOFcontent-sum(unigramcounts))	
		probabilitydist=[float(x)/float(sizeOFcontent) for x in unigramcounts]#probability distribution of a character chosen random from the file
		
		H =[-x*math.log(x,2) for x in probabilitydist]
		return sum(H),probabilitydist
	except:
		print "Error in code"
	finally:
			f.close()
					


The entropy H(Xn) =4.17 bits(3sf)

----Bigram Statistics---
2.

The aim of this excercise is to  compute the joint entropy H(Xn,Xn+1)
We know  H(Xn,Xn+1)= sum (-P(xn,xn+1)log(P(xn,xn+1)) where
P(xn,xn+1) is the probability of selecting a character xn randomly from a file and also reading the next character xn+1
Using the product rule P(xn,xn+1)= P(xn)P(xn+1|xn)
Now P(xn) as I have mentioned above in my code is:
		P(xn) = cn/N where cn is the number of time xn occurs in the file and N is the total number of alphabets in the file
Similarly 
	P(xn+1|xn) = wn/cn where  wn is the number of times xn and xn+1 occur together and the interpretation of cn is the smae as above
Therefore  P(xn)P(xn+1|xn) =cn/N *wn/cn = wn/N = P(xn,xn+1)
The code used to perform this computation is as follows:

Code:
#! /usr/bin/python
import re,math
def bigrammodel(file):
	f = open(file,'r')
	try: 
		content= f.read()
		letters =map(chr,range(97,123))
		letters.append(' ')
		bigramcounts =[]
		
		
		for l in letters:
			for l2 in letters:
				pattern=l+l2
				
				bigramcounts.append(len(re.findall(pattern,content)))
		print bigramcounts			
		sizeOFcontent= len(content)
		print sizeOFcontent		
		probabilitydist=[float(x)/float(sizeOFcontent) for x in bigramcounts]
		print probabilitydist
		print sum(probabilitydist)
		H =[-x*math.log(x,2) for x in probabilitydist if x!=0]
		return sum(H),probabilitydist
	except:
		print "Error in code"
	finally:
			f.close()	
	
a)
The join entrophy  H(Xn,Xn+1)=7.57 bits(3sf)

b)  H(Xn,Xn+1)< 2H(Xn) because the joint proabilities P(xn,xn+1) captures the context dependedency between the symbols. In English  for example letter 'u'; is more probable after 'q' than after 'e'. Hence the information content associated with observing a letter like 'u' at any point will change if  we take into account the knowledge gained from our previous observations.

c)
We know  H(Xn,Xn+1)= H(Xn) + H(Xn+1|Xn)
	Hence H(Xn+1|Xn)=  H(Xn,Xn+1)- H(Xn) =7.57- 4.17
					     = 3.40 bits(3sf)



---Compression with known distributions---
3.
For any probabilistic model H, the maximum number of bits used by an arithmetic encoder is equal to ceil[h(x|H)]+2 bits where ceil is the ceiling function and h(x|H) = log[1/P(x|H)]
a)
In the first instance, the characters are generated i.i.d with p(xn) using the unigram model. Therefore P(x|H) = prod_1^N[ P(x_i)] where prod_i^N corresponds the capital Pi notation representing the product of all p(xi) ranging from i=1 to i=N  where xi represents the ith alphabet in the file.
[Note : during the actual computing phase, multiplying large number of probabilities lead to numerical underflow. Hence to overcome this effect I have added the probabilities in log-space]
Therefore the maximum number of bits the arithmetic encoder might use to encode the file is ceil[log[1/P(x|H)]]+2= 1433836 bits

Code :
#! /usr/bin/python
import math
def arithmeticCodingUnf(file):
		f =open (file,'r')
		H,prodis =unigrammodel('thesis.txt')
		letters =map(chr,range(97,123))	
		letters.append(' ')
		print prodis		
		try:
			content = f.read()
			informationcontent=float(0);
			for i in range(len(content)):
				informationcontent= informationcontent- math.log(prodis[letters.index(content[i])],2)
			
			return informationcontent,prodis
		finally:
				f.close()
				

b)
In the second instance, the probabilistic model employed is more sophisticated. The first character is generated from p(xn) and all subsequent characters are generated from p(xn+1|xn) distribution. Therefore P(x|H) = p(x1)prod_i^N[ P(xi+1|xi)] (prod_i^N corresponds the capital Pi notation representing the product of all p(xi+1|xi) ranging from i=1 to i=N  where xi+1 and xi represents the i+1 and ith alphabet in the file.
The distribution of p(xn+1|xn) has been computed from the joint distibution p(xn,xn+1) ( computed in question 2) and marginal distribution p(xn)(computed in question 1) 
using Bayes rule: p(xn+1|xn)= p(xn,xn+1)/p(xn)
Therefore the maximum number of bits the arithmetic encoder might use to encode the file is ceil[log[1/P(x|H)]]+2= 1171230 bits

Code written:
#! /usr/bin/python
import math
def getconditiondist(size,index,joindis):
				partitions= len(joindis)/size
				return joindis[partitions*index:partitions*index+size]
				
				 
def arithmeticCodingbg(file):
		f =open (file,'r')
		H,prodis =unigrammodel('thesis.txt')#prodis= P(x) where x is a letter
		letters =map(chr,range(97,123))	
		letters.append(' ')	
		H2,bgprodis= bigrammodel(file)#bdprodis = P(x1,x2) the joint distributions
		try:
			content= f.read()
			informationcontent= math.log(prodis[letters.index(content[0])],2) #first character generated by p(xn)
			prev_l=content[0]
			content=content[1:]
			for i in range(len(content)):
				index= letters.index(content[i])
				index_prevl= letters.index(prev_l)
				joindis_letter = getconditiondist(27,index_prevl,bgprodis)
				conditional_prob =joindis_letter[index]/prodis[index_prevl]#P(xn,xn+1)/P(xn)
				information= math.log(conditional_prob,2) 
				informationcontent= informationcontent +information
				prev_l= content[i]
			return -informationcontent,bgprodis
		finally:
			f.close()



---Compression with Limited Header---

 4.
In real systems, senders are required to communicate their choice of models through encoding the description of the model in the header information. In the given scenario, the sender and receiver agree to employ the following suboptimal scheme: each probability pi generated from the  probabilistic model H is encoded to the next largest muliple 2^(-8) using 8 bits, qi= ceil(2^8*pi)/2^8 where ceil is the ceiling function. Inorder to ensure that these probabilities sum up to 1, the qi is renormalised: qi= qi/sum_i^n(qj) [sum_1^n corresponds to summation notation representing the sum of all qj where j ranges from 1 to n).Therefore for each probabilistic model, we now have an associated  approximate distribution.
	
Since the new distribution q(xn) is an approximation of p(xn), the extra number of bits used on average can be found using KL divergence: KL(p||q) = sum_1^n[pi * log(pi/qi)].
The information provided by the KL divergence can be used by the receiver to deduce the appropiate probabilistic model used during the encoding process. Lets consider the following scenario:
Suppose the sender decides to use the probability model H. The sender encodes the KL(p||q)*N (where p is generated from the model H, q is the approximate distribution and  N is the number of characters in the file) in the header. Hence if the sender uses model 1, the KL divergence is computed between the probabilty distribution p(xn) and its associated approximation  q(xn)= [ceil[2^8*p(xn)]/2^8]. Similarly if the sender decides to use the second model the KL divergence will be the probabilty distribution p(xn+1/xn) and its associated approximation  q(xn+1/xn)= [ceil[2^8*p(xn+1/xn)]/2^8]. 

The receiver can deduce  particular the probabilistic model H used by the arithmetic encoder by subtracting the subtracting the number of bits contained in the header information from number of bits used in the compressed data. The result will overlap exactly with the number of bits needed by the arithmetic encoder to compress the file when it used the probability model H. 

 
If the sender decides to use model 1, the number of bits used in the header is ceil(KL[p(xn)||q(xn)] *344026) =1245 bits, the number of bits used to compress the data is ceil[h(x|H)]+2 =1435081 bits  (we use the approximate distribution) and the size of the file is 1436326 bits

If the sender decides to use model 2, the number of bits used in the header is ceil(KL[p(xn+1|xn)||q(xn+1|xn)] *344026) = 7088 bits, the number of bits used to compress the data is ceil[h(x|H)]+2 = 1178318 bits (we use the approximate distribution) and the size of the file is 1185406 bits

Code written to compute Kl divergence:
#! /usr/bin/python
import math
def kullbackdiv(probdis1,probdis2):
	informationcontent=0
	for index,item in enumerate(probdis1):
		if item!=0:
			informationcontent+= item*math.log(item/probdis2[index],2)
	return informationcontent


Code written for Model 1:
#! /usr/bin/python
import math
def compressionLPH_Unf(file):
	f =open (file,'r')
	H,prodis =unigrammodel('thesis.txt')
	prodis = [ math.ceil(pow(2,8) *x)/pow(2,8) for x in prodis]
	prodis = [ x/sum(prodis) for x in prodis]
	letters =map(chr,range(97,123))	
	letters.append(' ')
	print prodis
	print sum(prodis)		
	try:
		content = f.read()
		informationcontent=float(0);
		L=[]
		for i in range(len(content)):
			informationcontent= informationcontent- math.log(prodis[letters.index(content[i])],2)
			
		return (informationcontent),prodis
	finally:
		f.close()

Code written Model 2:
#! /usr/bin/python
import math
def compressionLPH_Bg(file):	
	f =open (file,'r')
	H,prodis =unigrammodel('thesis.txt')#prodis= P(x) where x is a letter
	letters =map(chr,range(97,123))	
	letters.append(' ')	
	H2,bgprodis= bigrammodel(file)#bdprodis = P(x1,x2) the joint distributions
	
	try:
		content= f.read()
		informationcontent= math.log(prodis[letters.index(content[0])],2) #first character generated by p(xn)
		prev_l=content[0]
		content=content[1:]
		print len(content)
		for i in range(len(content)):
			index= letters.index(content[i])
			index_prevl= letters.index(prev_l)
			joindis_letter = getconditiondist(27,index_prevl,bgprodis)
			conditional_probdis =[x/prodis[index_prevl] for x in joindis_letter]#P(xn,xn+1)/P(xn)
			conditional_probdis=[math.ceil(pow(2,8) *x)/pow(2,8) for x in conditional_probdis]
			conditional_probdis = [ x/sum(conditional_probdis) for x in conditional_probdis ]
			information= math.log(conditional_probdis[index],2) 
			informationcontent= informationcontent +information
			prev_l= content[i]
		return math.ceil(-informationcontent),bgprodis
	finally:
		f.close()


---Compression with adaptation---

5.

An alternative to using header information is for both the sender and receiver to infer the distributions as they read or decode the file. 
Using the i.i.d model, the procedure that I followed to encode the text file is as follows:
	Step 1:
	Create a dictionary to hold counts for each letter. The letters act as keys and the values associated with each letter refers the number of times the letter has been seen so far.
	The product of probabilities is very small  and multiplying large number of probabilities leads to numerical underflow. To overcome this problem I have used log probabilities to store the probability of seeing the xn given the elements xk which we have already observed (k<n). 
	Step2:
	Therefore 

---Noisy channel Coding----

---X0R-ing packets-------
6.
Code written:
#! /usr/bin/python
import operator
def XOR():
	string="nutritious snacks"
	stringtoascii = [ord(c)  for c in string]
	stringtoascii=stringtoascii+ [59, 6,17 ,0,83,84,26,90,64,70,25,66,86,82,90,95,75]	
	return chr(reduce(operator.xor,stringtoascii))

The resutant ASCII string that is outputed:'['


---Decoding packets from a digital fountain----

7.
 Overview of the algorithm:

The steps performed by the algorithm are as follows:

Step1:
In step 1,the algorithm uses the information in 'received.txt' to create the list  'encodedPackets' and utilises the information in 'packets.txt' to create the list  'sourceBitsUsed'. 'encodedPackets' is a list of integers while 'sourceBitsUsed' is a list of list of integers but both lists have the same size. The ith  item in  encodedPackets[i] represents the ith encoded packet received and the ith list associated with sourceBitsUsed[i]  corresponds to  the source bits used to create this packet. Hence the indices of the lists serve as edges connecting  encoding packets with their appropiate source bit information.

Step 2:
In step 2, The algorithm  sequentially searches through each list in sourceBitsUsed to find the max integer 'm'. From the value of 'm', the algorithm can  then directly infer the number of source bits that it needs to deocde. 

Step3:


In the last step, the algorithm creates a list {s1,s2.....sm} (where si represents the source packet i) and executes the following loop until all si have been decoded.
Loop-
	while there exists s_i that is not decoded:
		for each list L in sourceBitsUsed:
			The algorithm checks whether the L contains 1 item. This check allows the program to locate encoded packets that are connected to only one source packet.  
			If the above check succeeds:
				i:Set the kth source alphabet(the value of k is given by the source packet) to the string chaacter defined by L's associated encoded packet.
				ii:Store L's associated encoded packet to local variable 'num' and for all lists in sourceBitsUsed that contains this source packet, apply bit wise modulo 2 addition between the corresponding encoded packet of these lists and num.
				iii: Remove the source packet from every list in sourcebitsUsed.(Step ii and Step iii ensures all edges connected L's sourcepacket has been removed)
	





Assumption made by the algorithm:
The algorothm assumes that the receiver has collected enough encoded packets to decode the original message.


The decoded String:
"Password: X!3baA1z"
The received packets that were used:[15, 22, 1, 20, 21, 19, 7, 9, 16, 18, 5, 6, 8, 10, 13, 14, 11, 12]
			   
Code:
#! /usr/bin/python
import operator
def LTdecode(file,file2):
	f=open(file,'r')
	g=open(file2,'r')
	encodedPackets=[]
	SourcebitsUsed=[]
	try:
		contents=f.readlines()
		for i in range(len(contents)):
			encodedPackets.append(int(contents[i].strip()))
		contentSource = g.readlines()
		for line in contentSource:
			line = line.strip().split()
			sourcebits=[int(x) for x in line]
			SourcebitsUsed.append(sourcebits)
		num_of_sourcebits=0
		for list in SourcebitsUsed:
			if max(list)>num_of_sourcebits:
				num_of_sourcebits = max(list)
		decodedS =['']*num_of_sourcebits
		PacketsU=[]
		while '' in decodedS:
			for index, list in enumerate(SourcebitsUsed):
				if len(list)==1 :
					sourcebit=list[0]
					p=SourcebitsUsed[index].pop()
					decodedS[sourcebit-1] =chr(encodedPackets[index])
					PacketsU.append(index)
					for j, lis in enumerate( SourcebitsUsed):
						if sourcebit in lis:
							lis.remove(sourcebit)
							encodedPackets[j] = operator.xor(encodedPackets[j],encodedPackets[index])
		print decodedS	,PacketsU
	finally:
		f.close()	
		g.close()


----Creating a Code----
8.

The error correcting code that I have employed for the last task is a combination of the Hamming(7,4) code and a repetition code.The description of the encoding and decoding algorithms is as follows:

Encoding Algorithm:

At the initial step, the encoding algorithm adds redundancy to blocks of data instead of encoding 1 bit at a time. The algorithm coverts a sequence of source bits 's' of length K=4 into a transmitted sequence 't' of length N=7. These N-K extra bits are linear functions of the source bits and serve as their parity checks. Before transmission through the binary symmetric channel, further redundancy is added to the transmitted sequence 't'. Each of the source bits is repeated three times hence increasing the length of the transmitted sequence to N=15. 

The code written to implement the encoding algorithm is as follows:
Code:
#! /usr/bin/python
import operator
def Encoder(sourcebits):
	R3code= sourcebits[0]*3+sourcebits[1]*3 + sourcebits[2]*3+sourcebits[3]*3
	s1=int(sourcebits[0])
	s2=int(sourcebits[1])
	s3=int(sourcebits[2])
	s4=int(sourcebits[3])
	s5 = reduce(operator.xor,[s1,s2,s3])
	s6 = reduce(operator.xor,[s2,s3,s4])
	s7= reduce(operator.xor,[s1,s3,s4])
	return R3code+str(s5)+str(s6)+str(s7) 	


This proposed error correcting code will fail to correct errors in situations where the noisy communication channel used to forward the transmitted sequence does not belong to the class of discrete memoryless channel. The decoding process of the proposed error correcting code assumes that the channel used is synchronized i.e the channel does not perform any insertions, deletions or permutations to the transmitted requence and noise is only added in the form of bit flips. As it is already stated in the question that the channel used is a BSC, which belongs to the class of discrete memoryless channels, it is hence safe to conclude that we are operating in a synchronized channel

Decoding algorithm:

The recieved sequence 'r' from the noist channel can be interpreted as a vector r where  r= t+n (t is the transmitted vector and n is the noise vector added by the channel). Upon receiving this vector r, the decoder attempts to remove and correct errors in two stages before outputting the most likley values of the original 4 source bits 's'.

Stage 1:
In the first stage, the algorithm reads the first 12 bits and partitions these bits in groups of three in a sequential manner.The groups generated will be of the form {r1,r2,r3} ,{r4,r5,r6} and so forth. If the channel introduced no error during transmission, then each group 'i' (where i belongs to {1,2,3,4}) will contain  copies of the single source bit s_i. To reduce the effect of noise, the algorithm extracts the most likely values of  each source bit s_i by performing a majority vote  on their respective groups.

Stage 2:
In the second stage, the algorithm reads the final three bits and uses these bits to achieve better estimates for the values of the source bits.This stage infact is exactly identical to the Hamming(7,4) decoding process. After conducting step 1, the decoder reduces the size of the received vector from 15 to 7 by removing redundant copies of the source bits and makes estimates of the values associated with the source bits through the use of majority votes. If all the noise introduced by the channel was removed in the stage 1, then this new reduce r vector will satisfy the equation r=G'*s (where G' represents the transpose of the 4 by 7 matrix mentioned in page 10 in Mackay) and the equation H*r=[0,0,0]( where each row of the matrix H represents a parity check).
However, the decoding process suffers from the same drawbacks as the hamming(7,4) decoder. The decoder fails to correct any m-bit error patterns where m>=2.
To correct one-bit error patterns, the algorithm uses the following methodology:

As I have already mentioned, if there were no errors, then r=t and Hr=[0,0,0] (in modulo 2 arithmetic)
Now in the case where r= t+n w(here n is one bit error pattern ie it is a vector of length 7 where only the ith entry is non-zero and set to 1)
 H*r= H*(t+n) = H*t +H*n
Since we already know H*t=[0 0 0] in modulo 2 arithmetic, therefore the product of H*r will be equal to  H*n whose product produces the ith column of the H matrix.There are seven possible one-bit error patterns that can be corrected by the decoder and each of these patterns have a one-to correpondence with the columns of the H matrix. Thus for example, if n=[0000100] then H*r will produce the 5th column of H, hence telling the algorithm to flip the 5th bit of vector r. This method is in fact optimal since it aims to flip the minimum number of bits such that the three parity checks are satisfied.
	


The code written to implement the decoding algorithm is as follows:
#! /usr/bin/python
import numpy
def Decoder(ReceivedBits):
	DecodedBits=[]
	#Stage 1: decoding the the four source bits using majority vote
	for i in range(4):
		seq= ReceivedBits[3*i:3*i+3]
		if len(re.findall('0',seq))>1:
			DecodedBits.append(0)
		else:
			DecodedBits.append(1)	
	#computing the syndrome z and using the parity checks to detect errors missed by the first stage
	t= numpy.array([DecodedBits[0],DecodedBits[1],DecodedBits[2],DecodedBits[3],int(ReceivedBits[-3]),int(ReceivedBits[-2]),int(ReceivedBits[-1])])
	H= numpy.array([[1,1,1,0,1,0,0],[0,1,1,1,0,1,0],[1,0,1,1,0,0,1]])
	z= numpy.dot(H,t)
	z =[ 2^x for x in z]
	columnsofH = [[1,0,1],[1,1,0],[1,1,1],[0,1,1],[1,0,0],[0,1,0],[0,0,1]]
	
	if z!=[0,0,0]:
		print 'yes'
		index = columnsofH.index(z)
		
		t[index]=(t[index]+1)^2
	return t[:4]	
		
		
	
